# -*- coding: utf-8 -*-
"""TechChallange-02

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E_42pk8C8ikFP2zTIn-Xe0HXYG5On1MM

#2º Tech Challange

#1 EDA
"""

import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
#Avisos
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/Dados Históricos - Ibovespa_4anos.csv", sep=",")
df.head(25)

"""Utilizamos a biblioteca "datetime" para a conversão da data de object para datetime corretamente. No outro formato estava convertanto uma parte dos dados para Ano/Mês/Dia e a outra parte para Ano/Dia/Mês."""

from datetime import datetime
def converterData(data):
  return datetime.strptime(data,'%d.%m.%Y')

df.head(25)

df['Data'] = df['Data'].apply(converterData)

df.info()

df.head(25)

df = df.rename(columns={"Vol.":"Vol", "Var%":"Var", "Mínima": "Minima", "Máxima": "Maxima", "Último":"Fechamento"})

"""Percebemosd um valor no ano de 2019 que representava mil com um K ao invés de Milhões com o M. Fizemos a multiplicação para melhor visualizar o dado na coluna Volume."""

def convert_to_float(value):
    if "M" in value:
        return float(value.replace("M", "").replace(",", ".")) * 1000000
    elif "K" in value:
        return float(value.replace("K", "").replace(",", ".")) * 1000
    else:
        return float(value.replace(",", "."))

df["Vol"] = df["Vol"].apply(convert_to_float)

print(df)

def convert_to_floats(value):
    return float(value.replace("%", "").replace(",", "."))

df["Var"] = df["Var"].apply(convert_to_floats)

print(df)

df.describe()

df = df.sort_values(by='Data')

df.head()

df.set_index('Data', inplace=True)

df.head()

df.info()

df.isnull().sum()

sns.boxplot(y="Fechamento", data=df, palette="hls")
plt.show()
sns.boxplot(y="Abertura", data=df, palette="hls")
plt.show()
sns.boxplot(y="Maxima", data=df, palette="hls")
plt.show()
sns.boxplot(y="Minima", data=df, palette="hls")
plt.show()
sns.boxplot(y="Vol", data=df, palette="hls")
plt.show()
sns.boxplot(y="Var", data=df, palette="hls")
plt.show()

sns.pairplot(df, vars=['Fechamento','Abertura','Maxima','Minima','Vol', "Var"])

plt.plot(df.index, df["Fechamento"])
plt.xlabel("Data")
plt.ylabel("Fechamento")
plt.title("Média de Fechamento ao Longo do Tempo")
plt.show()

plt.plot(df.index, df["Var"])
plt.xlabel("Data")
plt.ylabel("Volume médio de Invest.")
plt.title("Média de Vol. de Invest. ao Longo do Tempo")
plt.show()

plt.plot(df.index, df["Vol"])
plt.xlabel("Data")
plt.ylabel("Volume médio de Invest.")
plt.title("Média de Vol. de Invest. ao Longo do Tempo")
plt.show()

plt.plot(df.index, df["Minima"])
plt.xlabel("Data")
plt.ylabel("Volume médio de Invest.")
plt.title("Média de Vol. de Invest. ao Longo do Tempo")
plt.show()

correlation_matriz = df.corr().round(2)

fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(data = correlation_matriz, annot=True, linewidths=5, ax=ax)

plt.scatter(df.index, df["Vol"])
plt.xlabel("ultimo")
plt.ylabel("Volume de Investimento")
plt.title("Volume de investimento ao passar do tempo")
plt.show()

plt.scatter(df["Minima"], df["Vol"])
plt.xlabel("Maxima")
plt.ylabel("Volume de Investimento")
plt.title("Voume de investimento ao passar do tempo")
plt.show()

plt.bar(df.index, df["Fechamento"])
plt.xlabel("Data")
plt.ylabel("Voume de investimento")
plt.title("Volume de investimento ao passar do tempo")
plt.show()

print("Data Inicial:", df.index.min())

print("Data Final:", df.index.max())

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf

"""# 2 Seasonal Decompose
#*Tendência - Direção
#*Sazonalidade - Recorrência das oscilações
#*Resíduo - O que sobra do sinal
"""

dec_Vol = seasonal_decompose(df['Fechamento'], period=7)

fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(15,10))
dec_Vol.observed.plot(ax=ax1)
dec_Vol.trend.plot(ax=ax2)
dec_Vol.seasonal.plot(ax=ax3)
dec_Vol.resid.plot(ax=ax4)
plt.tight_layout()

dec_Max = seasonal_decompose(df['Maxima'], period=7)

fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(15,10))
dec_Max.observed.plot(ax=ax1)
dec_Max.trend.plot(ax=ax2)
dec_Max.seasonal.plot(ax=ax3)
dec_Max.resid.plot(ax=ax4)
plt.tight_layout()

"""#3 Intentificando Estacionária ou não Estacionária

## Teste ADF - (Augmented Dickey Fuller)

  H0 - Hipótese Nula (não Estacionária)
  
  H1 - Hipótese Alternativa (rejeição da hipótese nula)


p-value = 0.05 (5%) , então rejeitamos H0 com um nível de confiança de 95%
"""

from statsmodels.tsa.stattools import adfuller

df_dickeyF=df.Fechamento.values

R_dickeyF = adfuller(df_dickeyF)

print("teste ABF")
print(f"Teste Estatistico: {R_dickeyF[0]}")
print(f"p-value: {R_dickeyF[1]}")
print(f"Valores Críticos: ")

for key, value in R_dickeyF[4].items():
  print(f"\t{key}: {value}")

df_rol = df["Fechamento"].rolling(12).mean()

f, ax = plt.subplots()
df["Fechamento"].plot(ax=ax, legend=False)
df_rol.plot(ax=ax, legend=False, color="r")
plt.tight_layout()

"""#4 Forecasting"""

df_1=pd.read_csv("/content/Dados Históricos - Ibovespa_4anos.csv", sep=",")
df_1.head()

def convert_to_float(value):
    if "M" in value:
        return float(value.replace("M", "").replace(",", ".")) * 10000
    elif "K" in value:
        return float(value.replace("K", "").replace(",", ".")) * 10
    else:
        return float(value.replace(",", "."))

df_1["Vol."] = df_1["Vol."].apply(convert_to_float)

print(df_1)

df_1['Data'] = df_1['Data'].apply(converterData)

df_1.describe()

df_1.info()

df_1.rename(columns={"Vol.": "Volume"}, inplace=True)

df_1.head()

df_1 = df_1[["Data", "Último"]]
df_1 = df_1.rename(columns={"Data": "ds", "Último": "y"})

#Função gera um DataFrame com o periodo desejado
def fazer_dataFrame_periodo(DtInicio,DtFim):
  df = pd.DataFrame({'ds': pd.date_range(start=DtInicio, end=DtFim)})
  df['y'] = pd.Series([None] * len(df))

  return df.sort_values('ds',ascending = True)

df_faltantes = fazer_dataFrame_periodo(df_1['ds'].min(),df_1['ds'].max())

#Codigo abaixo da o merge da nova tabela criada com a tabela original
df_1 = pd.merge(df_faltantes,df_1, on='ds', how='outer')
df_1.drop('y_x',axis = 1,inplace=True)
df_1.rename(columns={'y_y':'y'},inplace = True)
#o metodo ffill vai preencher os dados com o valor anterior mais proximo, então como todos os valores faltando são de feriados e fins de semana o valor na bolsa fica 'Congelado' dai utilizo o do dia anterior
df_1.fillna(method='ffill',inplace  = True)

df_1

df_1.info()

!pip install statsforecast

df_1.tail()

df_1["unique_id"] = "Ibovespa"

df_1.head()

treino = df_1.loc[df_1["ds"]<"2022-10-01"]
valid = df_1.loc[(df_1["ds"]>="2022-10-01")&(df_1["ds"]<"2023-02-18")]
h = valid["ds"].nunique() #validação do período "valid" (3meses)

treino.head()

h

def wmape(y_true, y_pred):
  return np.abs(y_true-y_pred).sum()/np.abs(y_true).sum()

from statsforecast import StatsForecast
from statsforecast.models import Naive, SeasonalNaive, SeasonalWindowAverage, AutoARIMA

model = StatsForecast(models=[Naive()], freq="D", n_jobs=-1)
model.fit(treino)

forecast_df = model.predict(h=h, level=[90])
forecast_df = forecast_df.reset_index().merge(valid, on=["ds", "unique_id"], how="left")

errors = abs(forecast_df["y"] - forecast_df["Naive"])
wmape_numerator = sum(errors) / sum(forecast_df["y"])
wmape = 100 * wmape_numerator / len(forecast_df["y"])
print(f"WMAPE: {wmape:.2%}")

model.plot(treino, forecast_df, level=[90], unique_ids=["Ibovespa"], engine="matplotlib")

"""Forecasting - testamos o modelo, mas apresentou uma predição estatica com um unico valor repetido formando uma linha reta.

#5 Scikt learn.
"""

df.head()

df.info()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

X = df.drop(columns=['Fechamento'])
y = df['Fechamento']

# Dividir os dados em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar e treinar o modelo de regressão linear
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Realizar previsões no conjunto de teste
y_pred = modelo.predict(X_test)

# Avaliar a acurácia do modelo usando o coeficiente de determinação (R²)
acuracia = r2_score(y_test, y_pred)
print("Acurácia do Modelo (R²):", acuracia)

"""Scikt Learn - testamos o modelo que apresentou uma resultado muito proximo de 100% o que nos mostra um overfiting.

#6 stats Model
"""

import pandas as pd
import statsmodels.api as sm

X = df.drop(columns=['Fechamento'])
y = df['Fechamento']

# Adicionar uma coluna de constante aos atributos (intercepto)
X = sm.add_constant(X)

# Dividir os dados em conjunto de treinamento e teste (opcional, depende do caso)
# Aqui você pode usar a mesma abordagem de divisão de dados que mencionamos anteriormente

# Criar o modelo de regressão
modelo = sm.OLS(y, X).fit()

# Obter as previsões do modelo
y_pred = modelo.predict(X)

# Calcular a acurácia (coeficiente de determinação R²)
acuracia = modelo.rsquared
print("Acurácia do Modelo (R²):", acuracia)

"""Stats Model - testamos o modelo que apresentou uma resultado muito proximo de 100% o que nos mostra um overfiting.

#7 PROPHET - (validação do modelo)
https://facebook.github.io/prophet/docs/quick_start.html#python-api
"""

from prophet import Prophet

df_prof=pd.read_csv("/content/Dados Históricos - Ibovespa_4anos.csv", sep=",")
df_prof.head()

df_prof['Data'] = df_prof['Data'].apply(converterData)

df_prof = df_prof[['Data', 'Último']]
df_prof

df_prof.rename(columns={'Data': 'ds', "Último":"y"}, inplace=True)
df_prof

df_prof1 = Prophet()
df_prof1.fit(df_prof)

future = df_prof1.make_future_dataframe(periods=1095)
future.tail()

forecast = df_prof1.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1 = df_prof1.plot(forecast)

fig2 = df_prof1.plot_components(forecast)

from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(df_prof1, forecast)

plot_components_plotly(df_prof1, forecast)

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd

from prophet.diagnostics import performance_metrics
from prophet.diagnostics import cross_validation
df_cv = cross_validation(df_prof1, initial='395 days', period='180 days', horizon = '1000 days')

df_cv.head()

df_p = performance_metrics(df_cv)
df_p.head(25)

from prophet.plot import plot_cross_validation_metric
fig = plot_cross_validation_metric(df_cv, metric='mape')

"""Prophet - Percebemos que o modelo ficou mais proximo da realizade quando fizemos a validação da predição. Porém poor ser um dado sensivel optamos por trabalharmos com uma variação menor para a predição. Por isso trazemos no proximo passo uma variação do prophet com uma predição de 7 dias.

#8 PROPHET - APERFEIÇOAMENTO DO MODELO
"""

import pandas as pd
import numpy as np
from prophet import Prophet
from sklearn import metrics


#Avisos
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("/content/Dados Históricos - Ibovespa_4anos.csv", sep=",")
df.head()

from datetime import datetime
def converterData(data):
  return datetime.strptime(data,'%d.%m.%Y')

df['Data'] = df['Data'].apply(converterData)

df = df[['Data','Último']]
df = df.rename(columns={'Data':'ds','Último':'y'})
df = df.sort_values('ds',ascending= True)

def run_prophet(train,periodo):
    prophet = Prophet()
    prophet.fit(train)

    future = prophet.make_future_dataframe(periodo, include_history=False, freq='D')
    p = prophet.predict(future)

    return p,prophet

#Testando previsão dos proximos 7 dias utilizando como janela os ultimos 60 dias
train = df.loc[(df["ds"]>='2023-06-11') & (df["ds"]<"2023-08-11")].sort_values('ds', ascending = True)
test  = df.loc[(df["ds"]>='2023-08-11') & (df["ds"] <= '2023-08-18')].sort_values('ds', ascending = True)

p,prophet = run_prophet(train,16)

#Removendo as datas que não seram necessarias para validação,pois, trata-se de datas de fim de semana e ou feriados.
p = p[p['ds'].isin(test['ds'])]

p[['ds','yhat']].tail(5)

test.tail(5)

print('MAE: {}'.format(metrics.mean_absolute_error(test['y'].values,p['yhat'].values)))
print('RMSE: {}'.format(metrics.mean_squared_error(test['y'].values,p['yhat'])))
print('MAPE: {}'.format(metrics.mean_absolute_percentage_error(test['y'].values,p['yhat'].values)))

p.set_index("ds")['yhat'].plot(color='red')
test.set_index("ds")['y'].plot(color='blue')

from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(prophet, p)

fig2 = prophet.plot_components(p)

"""#Fizemos várias análises com diferentes volumes de dados e chegamos ao melhor resultado. Ao analisarmos com uma predição de 7 dias nós percebemos um erro de 1.4% apenas no modelo. Então assim entendemos que o modelo prophet com uma predição de 7 dias é o melhor modelo analisado pelo grupo."""